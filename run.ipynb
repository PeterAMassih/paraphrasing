{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrase Generation and Evaluation: From Dataset Creation to Model Fine-Tuning\n",
    "\n",
    "This notebook walks through my comprehensive project on paraphrase generation and evaluation. I'll cover the entire pipeline from collecting data, generating paraphrases, training/utilizing specialized models, and evaluating their performance across different types of text and benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal was to fine-tune specialized models that can generate paraphrases that:\n",
    "\n",
    "Preserve the original meaning\n",
    "use diverse vocabulary and structure, and\n",
    "Maintain the style appropriate to different text domains\n",
    "\n",
    "To achieve this:\n",
    "\n",
    "1. Created a diverse dataset from multiple domains (literary, technical, academic, article)\n",
    "2. Generated high-quality paraphrases using LLMs\n",
    "3. Fine-tuned using LoRA on DeepSeek-R1-Distill-Qwen-7B, utilized eugenesiow/bart-paraphrase and mrm8488/t5-small-finetuned-quora-for-paraphrasing (the last two were used as baseline)\n",
    "4. Evaluated with multiple metrics (BertScore, inverseBleu ...)\n",
    "5. Compared performance across different models and domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18218,
     "status": "ok",
     "timestamp": 1747807188163,
     "user": {
      "displayName": "Peter A. Massih",
      "userId": "02579015314160660239"
     },
     "user_tz": 240
    },
    "id": "HqJmjKS9aI_G",
    "outputId": "ecbc7b22-ba0d-49b7-f362-1dd236877d5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# If using colab and the codebase is there\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 334,
     "status": "ok",
     "timestamp": 1747807188499,
     "user": {
      "displayName": "Peter A. Massih",
      "userId": "02579015314160660239"
     },
     "user_tz": 240
    },
    "id": "wqWrZVqVan6c",
    "outputId": "09166150-169b-4524-bea0-12aa96f6681b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Paraphrasing/Swisscome\n"
     ]
    }
   ],
   "source": [
    "# Navigate where the dir is downloaded\n",
    "# %cd /content/drive/MyDrive/Paraphrasing/Swisscome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74686,
     "status": "ok",
     "timestamp": 1747807263187,
     "user": {
      "displayName": "Peter A. Massih",
      "userId": "02579015314160660239"
     },
     "user_tz": 240
    },
    "id": "J9YESOi3bO2i",
    "outputId": "2a6e22ad-5b9b-4dc3-dc7b-0d05e3abe5d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m129.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19511,
     "status": "ok",
     "timestamp": 1747807282735,
     "user": {
      "displayName": "Peter A. Massih",
      "userId": "02579015314160660239"
     },
     "user_tz": 240
    },
    "id": "33zSqHEQbTSO",
    "outputId": "9bd26b7a-e102-4b2c-eb15-577cf0886dc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA A100-SXM4-40GB\n",
      "GPU memory: 42.47 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import random\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Collection and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1  Sentence Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step was to collect a diverse set of sentences from various domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences collected: 212\n",
      "  - literary: 125 sentences (59.0%)\n",
      "  - technical: 2 sentences (0.9%)\n",
      "  - academic: 57 sentences (26.9%)\n",
      "  - article: 28 sentences (13.2%)\n"
     ]
    }
   ],
   "source": [
    "# Let's load our collected sentences\n",
    "with open(\"data/evaluation/collected_sentences.json\", \"r\") as f:\n",
    "    collected_sentences = json.load(f)\n",
    "\n",
    "domain_counts = {domain: len(sentences) for domain, sentences in collected_sentences.items()}\n",
    "total_sentences = sum(domain_counts.values())\n",
    "\n",
    "print(f\"Total sentences collected: {total_sentences}\")\n",
    "for domain, count in domain_counts.items():\n",
    "    print(f\"  - {domain}: {count} sentences ({count/total_sentences*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our collection process used various sources:\n",
    "\n",
    "- Literary: Sentences from classic novels like \"The Great Gatsby\" and \"Pride and Prejudice\"\n",
    "- Technical: Documentation from GitHub repositories including React and TensorFlow\n",
    "- Academic: Research papers including \"Attention is All You Need\" and \"DeepSeek\" publications\n",
    "- Article: Contemporary articles on AI and technology trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Preprocessing\n",
    "\n",
    "Before using these sentences, we applied preprocessing:\n",
    "\n",
    "- Removing extra whitespace\n",
    "- Normalizing punctuation\n",
    "- Ensuring proper sentence endings\n",
    "- Filtering by minimum word count (30+)\n",
    "\n",
    "This ensured our dataset contained substantial, well-formed sentences that would be challenging to paraphrase while preserving meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Paraphrase Generation with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our gold-standard paraphrases, we used o4-mini to generate three distinct paraphrases for each sentence while preserving the original meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items: 212\n",
      "Items with 3 paraphrases: 212 (100.0%)\n",
      "\n",
      "Example from literary domain:\n",
      "Original: Mr. Collins no sooner saw the two girls than he began to congratulate them on their good fortune, which Charlotte explained by letting them know that the whole party was asked to dine at Rosings the next day.\n",
      "Paraphrase 1: Hardly had Mr. Collins caught sight of the two girls before he was congratulating them on their good fortune, which Charlotte then clarified by informing them that the entire party had been invited to dine at Rosings the next day.\n",
      "Paraphrase 2: Immediately upon seeing the two girls, Mr. Collins congratulated them on their luck, a gesture Charlotte elucidated by revealing that the whole party was invited to dinner at Rosings the following day.\n",
      "Paraphrase 3: No sooner did Mr. Collins set eyes on the two girls than he began praising their good fortune, an explanation Charlotte provided by announcing that the entire party had been asked to dine at Rosings the next day.\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/evaluation/paraphrase_dataset.json\", \"r\") as f:\n",
    "    paraphrase_dataset = json.load(f)\n",
    "\n",
    "complete_paraphrases = 0\n",
    "total_items = 0\n",
    "\n",
    "for domain, items in paraphrase_dataset.items():\n",
    "    for item in items:\n",
    "        total_items += 1\n",
    "        if \"paraphrases\" in item and len(item[\"paraphrases\"]) == 3:\n",
    "            complete_paraphrases += 1\n",
    "\n",
    "print(f\"Total items: {total_items}\")\n",
    "print(f\"Items with 3 paraphrases: {complete_paraphrases} ({complete_paraphrases/total_items*100:.1f}%)\")\n",
    "\n",
    "# Let's look at a random example\n",
    "random_domain = random.choice(list(paraphrase_dataset.keys()))\n",
    "random_item = random.choice(paraphrase_dataset[random_domain])\n",
    "\n",
    "print(f\"\\nExample from {random_domain} domain:\")\n",
    "print(f\"Original: {random_item['text']}\")\n",
    "if \"paraphrases\" in random_item:\n",
    "    for i, para in enumerate(random_item[\"paraphrases\"], 1):\n",
    "        print(f\"Paraphrase {i}: {para}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paraphrase generation prompt was carefully designed to:\n",
    "\n",
    "- Preserve the original meaning\n",
    "- Create diverse vocabulary and structure\n",
    "- Maintain the appropriate tone and complexity\n",
    "- Generate three distinct variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Train/Test Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our dataset into training (90%) and testing (10%) sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 190 sentences (89.6%)\n",
      "Test set: 22 sentences (10.4%)\n",
      "\n",
      "Domain distribution:\n",
      "  - literary: 58.4% in train, 63.6% in test\n",
      "  - article: 13.2% in train, 13.6% in test\n",
      "  - academic: 27.4% in train, 22.7% in test\n",
      "  - technical: 1.1% in train, 0.0% in test\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/evaluation/splits/paraphrase_train.json\", \"r\") as f:\n",
    "        train_data = json.load(f)\n",
    "    \n",
    "with open(\"data/evaluation/splits/paraphrase_test.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "train_counts = {domain: len(sentences) for domain, sentences in train_data.items()}\n",
    "test_counts = {domain: len(sentences) for domain, sentences in test_data.items()}\n",
    "\n",
    "total_train = sum(train_counts.values())\n",
    "total_test = sum(test_counts.values())\n",
    "\n",
    "print(f\"Train set: {total_train} sentences ({total_train/(total_train+total_test)*100:.1f}%)\")\n",
    "print(f\"Test set: {total_test} sentences ({total_test/(total_train+total_test)*100:.1f}%)\")\n",
    "\n",
    "# Domain distribution in train/test\n",
    "train_dist = {domain: count/total_train*100 for domain, count in train_counts.items()}\n",
    "test_dist = {domain: count/total_test*100 for domain, count in test_counts.items()}\n",
    "\n",
    "print(\"\\nDomain distribution:\")\n",
    "for domain in train_counts.keys():\n",
    "    print(f\"  - {domain}: {train_dist[domain]:.1f}% in train, {test_dist.get(domain, 0):.1f}% in test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze our dataset to understand the characteristics of sentences across domains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence statistics by domain (word count):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_words</th>\n",
       "      <th>median_words</th>\n",
       "      <th>min_words</th>\n",
       "      <th>max_words</th>\n",
       "      <th>std_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>literary</th>\n",
       "      <td>47.216000</td>\n",
       "      <td>41.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>17.175603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>technical</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>academic</th>\n",
       "      <td>41.315789</td>\n",
       "      <td>37.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>13.847741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article</th>\n",
       "      <td>41.285714</td>\n",
       "      <td>37.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>16.161809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           mean_words  median_words  min_words  max_words  std_words\n",
       "literary    47.216000          41.0       30.0      126.0  17.175603\n",
       "technical   32.000000          32.0       32.0       32.0   0.000000\n",
       "academic    41.315789          37.0       30.0       89.0  13.847741\n",
       "article     41.285714          37.0       30.0      116.0  16.161809"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "domain_stats = {}\n",
    "\n",
    "for domain, items in collected_sentences.items():\n",
    "    lengths = [len(word_tokenize(item[\"text\"])) for item in items]\n",
    "    domain_stats[domain] = {\n",
    "        \"mean_words\": np.mean(lengths),\n",
    "        \"median_words\": np.median(lengths),\n",
    "        \"min_words\": np.min(lengths),\n",
    "        \"max_words\": np.max(lengths),\n",
    "        \"std_words\": np.std(lengths)\n",
    "    }\n",
    "\n",
    "stats_df = pd.DataFrame(domain_stats).T\n",
    "print(\"Sentence statistics by domain (word count):\")\n",
    "display(stats_df)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for domain in collected_sentences.keys():\n",
    "    lengths = [len(word_tokenize(item[\"text\"])) for item in collected_sentences[domain]]\n",
    "    plt.hist(lengths, alpha=0.6, label=domain, bins=20)\n",
    "\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Sentence Length Distribution by Domain')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different domains have characteristic sentence structures and styles:\n",
    "\n",
    "- Literary texts often have more varied sentence structures with descriptive language\n",
    "- Technical documentation tends to be more concise and precise\n",
    "- Academic text contains specialized terminology and complex constructions\n",
    "- Articles balance readability with information density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Fine-tuning DeepSeek with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fine-tuned DeepSeek-R1-Distill-Qwen-7B with LoRA adapters for paraphrase generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training prompt format for DeepSeek-LoRA:\n",
      "\n",
      "\"messages\": [\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Paraphrase the following text while preserving its meaning but changing the wording and structure: [original]\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"<think>\\nLet me analyze this text and find ways to rephrase it while keeping the same meaning.\\nI need to use different vocabulary and structure.\\n</think>\\n\\n[paraphrase]\"\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt format explanation\n",
    "print(\"Training prompt format for DeepSeek-LoRA:\")\n",
    "print('''\n",
    "\"messages\": [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Paraphrase the following text while preserving its meaning but changing the wording and structure: [original]\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"<think>\\\\nLet me analyze this text and find ways to rephrase it while keeping the same meaning.\\\\nI need to use different vocabulary and structure.\\\\n</think>\\\\n\\\\n[paraphrase]\"\n",
    "    }\n",
    "]\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 737673,
     "status": "ok",
     "timestamp": 1747809984553,
     "user": {
      "displayName": "Peter A. Massih",
      "userId": "02579015314160660239"
     },
     "user_tz": 240
    },
    "id": "kZzD6-bEbXG8",
    "outputId": "dfb1007a-b09e-444a-a05b-5f7b6cf866c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-21 06:34:12.275953: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747809252.297318   15762 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747809252.303835   15762 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Loading deepseek-ai/DeepSeek-R1-Distill-Qwen-7B...\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.12s/it]\n",
      "Adding LoRA adapters...\n",
      "trainable params: 575,275,008 || all params: 8,190,891,520 || trainable%: 7.0234\n",
      "Loading datasets...\n",
      "Training dataset size: 2,570\n",
      "Evaluation dataset size: 122\n",
      "Initializing SFTTrainer...\n",
      "Tokenizing train dataset: 100% 2570/2570 [00:00<00:00, 2733.93 examples/s]\n",
      "Truncating train dataset: 100% 2570/2570 [00:00<00:00, 424167.21 examples/s]\n",
      "Converting eval dataset to ChatML: 100% 122/122 [00:00<00:00, 12328.76 examples/s]\n",
      "Applying chat template to eval dataset: 100% 122/122 [00:00<00:00, 3147.64 examples/s]\n",
      "Tokenizing eval dataset: 100% 122/122 [00:00<00:00, 2514.66 examples/s]\n",
      "Truncating eval dataset: 100% 122/122 [00:00<00:00, 43022.12 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Starting fine-tuning...\n",
      "  5% 24/480 [00:24<07:17,  1.04it/s]\n",
      "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6% 2/31 [00:00<00:01, 18.27it/s]\u001b[A\n",
      " 13% 4/31 [00:00<00:02, 12.41it/s]\u001b[A\n",
      " 19% 6/31 [00:00<00:02, 12.05it/s]\u001b[A\n",
      " 26% 8/31 [00:00<00:01, 12.05it/s]\u001b[A\n",
      " 32% 10/31 [00:00<00:01, 12.14it/s]\u001b[A\n",
      " 39% 12/31 [00:00<00:01, 12.09it/s]\u001b[A\n",
      " 45% 14/31 [00:01<00:01, 12.05it/s]\u001b[A\n",
      " 52% 16/31 [00:01<00:01, 12.00it/s]\u001b[A\n",
      " 58% 18/31 [00:01<00:01, 11.97it/s]\u001b[A\n",
      " 65% 20/31 [00:01<00:00, 12.00it/s]\u001b[A\n",
      " 71% 22/31 [00:01<00:00, 12.00it/s]\u001b[A\n",
      " 77% 24/31 [00:01<00:00, 11.82it/s]\u001b[A\n",
      " 84% 26/31 [00:02<00:00, 11.76it/s]\u001b[A\n",
      " 90% 28/31 [00:02<00:00, 11.74it/s]\u001b[A\n",
      "                                    \n",
      "\u001b[A{'eval_loss': 1.6209028959274292, 'eval_runtime': 2.6976, 'eval_samples_per_second': 45.225, 'eval_steps_per_second': 11.492, 'eval_num_tokens': 23164.0, 'eval_mean_token_accuracy': 0.6989978244227748, 'epoch': 0.15}\n",
      "  5% 24/480 [00:27<07:17,  1.04it/s]\n",
      "100% 31/31 [00:02<00:00, 11.77it/s]\u001b[A\n",
      " 10% 48/480 [00:50<06:56,  1.04it/s]\n",
      "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6% 2/31 [00:00<00:01, 18.32it/s]\u001b[A\n",
      " 13% 4/31 [00:00<00:02, 12.33it/s]\u001b[A\n",
      " 19% 6/31 [00:00<00:02, 11.93it/s]\u001b[A\n",
      " 26% 8/31 [00:00<00:01, 11.87it/s]\u001b[A\n",
      " 32% 10/31 [00:00<00:01, 11.83it/s]\u001b[A\n",
      " 39% 12/31 [00:00<00:01, 11.80it/s]\u001b[A\n",
      " 45% 14/31 [00:01<00:01, 11.86it/s]\u001b[A\n",
      " 52% 16/31 [00:01<00:01, 11.87it/s]\u001b[A\n",
      " 58% 18/31 [00:01<00:01, 11.86it/s]\u001b[A\n",
      " 65% 20/31 [00:01<00:00, 11.92it/s]\u001b[A\n",
      " 71% 22/31 [00:01<00:00, 11.94it/s]\u001b[A\n",
      " 77% 24/31 [00:01<00:00, 11.95it/s]\u001b[A\n",
      " 84% 26/31 [00:02<00:00, 11.96it/s]\u001b[A\n",
      " 90% 28/31 [00:02<00:00, 11.96it/s]\u001b[A\n",
      "                                    \n",
      "\u001b[A{'eval_loss': 1.3850141763687134, 'eval_runtime': 2.6758, 'eval_samples_per_second': 45.594, 'eval_steps_per_second': 11.585, 'eval_num_tokens': 46934.0, 'eval_mean_token_accuracy': 0.7270170277164828, 'epoch': 0.3}\n",
      " 10% 48/480 [00:53<06:56,  1.04it/s]\n",
      "100% 31/31 [00:02<00:00, 12.00it/s]\u001b[A\n",
      "{'loss': 2.1997, 'grad_norm': 3.216397285461426, 'learning_rate': 0.00019852039363842393, 'num_tokens': 48698.0, 'mean_token_accuracy': 0.6314826475083828, 'epoch': 0.31}\n",
      " 15% 72/480 [01:35<06:49,  1.00s/it]\n",
      "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6% 2/31 [00:00<00:01, 18.24it/s]\u001b[A\n",
      " 13% 4/31 [00:00<00:02, 12.34it/s]\u001b[A\n",
      " 19% 6/31 [00:00<00:02, 12.02it/s]\u001b[A\n",
      " 26% 8/31 [00:00<00:01, 12.02it/s]\u001b[A\n",
      " 32% 10/31 [00:00<00:01, 12.06it/s]\u001b[A\n",
      " 39% 12/31 [00:00<00:01, 12.09it/s]\u001b[A\n",
      " 45% 14/31 [00:01<00:01, 12.07it/s]\u001b[A\n",
      " 52% 16/31 [00:01<00:01, 12.05it/s]\u001b[A\n",
      " 58% 18/31 [00:01<00:01, 12.05it/s]\u001b[A\n",
      " 65% 20/31 [00:01<00:00, 12.03it/s]\u001b[A\n",
      " 71% 22/31 [00:01<00:00, 12.03it/s]\u001b[A\n",
      " 77% 24/31 [00:01<00:00, 12.01it/s]\u001b[A\n",
      " 84% 26/31 [00:02<00:00, 11.99it/s]\u001b[A\n",
      " 90% 28/31 [00:02<00:00, 11.95it/s]\u001b[A\n",
      "                                    \n",
      "\u001b[A{'eval_loss': 1.3949997425079346, 'eval_runtime': 2.6719, 'eval_samples_per_second': 45.661, 'eval_steps_per_second': 11.602, 'eval_num_tokens': 71677.0, 'eval_mean_token_accuracy': 0.7280911178358139, 'epoch': 0.45}\n",
      " 15% 72/480 [01:38<06:49,  1.00s/it]\n",
      "100% 31/31 [00:02<00:00, 11.96it/s]\u001b[A\n",
      " 20% 96/480 [02:03<06:16,  1.02it/s]\n",
      "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6% 2/31 [00:00<00:01, 18.38it/s]\u001b[A\n",
      " 13% 4/31 [00:00<00:02, 12.43it/s]\u001b[A\n",
      " 19% 6/31 [00:00<00:02, 12.05it/s]\u001b[A\n",
      " 26% 8/31 [00:00<00:01, 11.95it/s]\u001b[A\n",
      " 32% 10/31 [00:00<00:01, 11.82it/s]\u001b[A\n",
      " 39% 12/31 [00:00<00:01, 11.67it/s]\u001b[A\n",
      " 45% 14/31 [00:01<00:01, 11.63it/s]\u001b[A\n",
      " 52% 16/31 [00:01<00:01, 11.59it/s]\u001b[A\n",
      " 58% 18/31 [00:01<00:01, 11.45it/s]\u001b[A\n",
      " 65% 20/31 [00:01<00:00, 11.44it/s]\u001b[A\n",
      " 71% 22/31 [00:01<00:00, 11.47it/s]\u001b[A\n",
      " 77% 24/31 [00:02<00:00, 11.47it/s]\u001b[A\n",
      " 84% 26/31 [00:02<00:00, 11.49it/s]\u001b[A\n",
      " 90% 28/31 [00:02<00:00, 11.39it/s]\u001b[A\n",
      "                                    \n",
      "\u001b[A{'eval_loss': 1.3999707698822021, 'eval_runtime': 2.7716, 'eval_samples_per_second': 44.018, 'eval_steps_per_second': 11.185, 'eval_num_tokens': 93640.0, 'eval_mean_token_accuracy': 0.7282424242265763, 'epoch': 0.6}\n",
      " 20% 96/480 [02:05<06:16,  1.02it/s]\n",
      "100% 31/31 [00:02<00:00, 11.31it/s]\u001b[A\n",
      "{'loss': 1.323, 'grad_norm': 2.525120735168457, 'learning_rate': 0.0001869449552616367, 'num_tokens': 98240.0, 'mean_token_accuracy': 0.7334498834609985, 'epoch': 0.62}\n",
      " 25% 120/480 [02:40<06:28,  1.08s/it]\n",
      "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6% 2/31 [00:00<00:01, 18.20it/s]\u001b[A\n",
      " 13% 4/31 [00:00<00:02, 11.43it/s]\u001b[A\n",
      " 19% 6/31 [00:00<00:02, 10.41it/s]\u001b[A\n",
      " 26% 8/31 [00:00<00:02, 10.72it/s]\u001b[A\n",
      " 32% 10/31 [00:00<00:01, 10.68it/s]\u001b[A\n",
      " 39% 12/31 [00:01<00:01, 10.34it/s]\u001b[A\n",
      " 45% 14/31 [00:01<00:01, 10.85it/s]\u001b[A\n",
      " 52% 16/31 [00:01<00:01, 11.22it/s]\u001b[A\n",
      " 58% 18/31 [00:01<00:01, 11.50it/s]\u001b[A\n",
      " 65% 20/31 [00:01<00:00, 11.71it/s]\u001b[A\n",
      " 71% 22/31 [00:01<00:00, 11.83it/s]\u001b[A\n",
      " 77% 24/31 [00:02<00:00, 11.90it/s]\u001b[A\n",
      " 84% 26/31 [00:02<00:00, 11.82it/s]\u001b[A\n",
      " 90% 28/31 [00:02<00:00, 11.59it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 1.40309739112854, 'eval_runtime': 2.817, 'eval_samples_per_second': 43.308, 'eval_steps_per_second': 11.005, 'eval_num_tokens': 117352.0, 'eval_mean_token_accuracy': 0.7308069505999165, 'epoch': 0.75}\n",
      " 25% 120/480 [02:43<06:28,  1.08s/it]\n",
      "100% 31/31 [00:02<00:00, 11.51it/s]\u001b[A\n",
      " 30% 144/480 [03:08<05:30,  1.02it/s]\n",
      "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6% 2/31 [00:00<00:01, 18.39it/s]\u001b[A\n",
      " 13% 4/31 [00:00<00:02, 12.43it/s]\u001b[A\n",
      " 19% 6/31 [00:00<00:02, 12.07it/s]\u001b[A\n",
      " 26% 8/31 [00:00<00:01, 12.06it/s]\u001b[A\n",
      " 32% 10/31 [00:00<00:01, 12.07it/s]\u001b[A\n",
      " 39% 12/31 [00:00<00:01, 12.02it/s]\u001b[A\n",
      " 45% 14/31 [00:01<00:01, 11.97it/s]\u001b[A\n",
      " 52% 16/31 [00:01<00:01, 11.96it/s]\u001b[A\n",
      " 58% 18/31 [00:01<00:01, 11.97it/s]\u001b[A\n",
      " 65% 20/31 [00:01<00:00, 12.00it/s]\u001b[A\n",
      " 71% 22/31 [00:01<00:00, 12.07it/s]\u001b[A\n",
      " 77% 24/31 [00:01<00:00, 12.07it/s]\u001b[A\n",
      " 84% 26/31 [00:02<00:00, 12.07it/s]\u001b[A\n",
      " 90% 28/31 [00:02<00:00, 12.07it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 1.4301320314407349, 'eval_runtime': 2.6497, 'eval_samples_per_second': 46.043, 'eval_steps_per_second': 11.699, 'eval_num_tokens': 140261.0, 'eval_mean_token_accuracy': 0.7361539160051653, 'epoch': 0.9}\n",
      " 30% 144/480 [03:10<05:30,  1.02it/s]\n",
      "100% 31/31 [00:02<00:00, 12.14it/s]\u001b[A\n",
      "{'loss': 1.1822, 'grad_norm': 2.330162525177002, 'learning_rate': 0.0001651541093470229, 'num_tokens': 146114.0, 'mean_token_accuracy': 0.7574102824926376, 'epoch': 0.93}\n",
      " 35% 168/480 [03:51<05:34,  1.07s/it]\n",
      "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6% 2/31 [00:00<00:01, 18.38it/s]\u001b[A\n",
      " 13% 4/31 [00:00<00:02, 12.40it/s]\u001b[A\n",
      " 19% 6/31 [00:00<00:02, 12.11it/s]\u001b[A\n",
      " 26% 8/31 [00:00<00:01, 12.09it/s]\u001b[A\n",
      " 32% 10/31 [00:00<00:01, 12.06it/s]\u001b[A\n",
      " 39% 12/31 [00:00<00:01, 11.98it/s]\u001b[A\n",
      " 45% 14/31 [00:01<00:01, 11.95it/s]\u001b[A\n",
      " 52% 16/31 [00:01<00:01, 11.80it/s]\u001b[A\n",
      " 58% 18/31 [00:01<00:01, 11.77it/s]\u001b[A\n",
      " 65% 20/31 [00:01<00:00, 11.61it/s]\u001b[A\n",
      " 71% 22/31 [00:01<00:00, 11.70it/s]\u001b[A\n",
      " 77% 24/31 [00:02<00:00, 11.68it/s]\u001b[A\n",
      " 84% 26/31 [00:02<00:00, 11.75it/s]\u001b[A\n",
      " 90% 28/31 [00:02<00:00, 11.70it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 1.5142183303833008, 'eval_runtime': 2.6946, 'eval_samples_per_second': 45.276, 'eval_steps_per_second': 11.505, 'eval_num_tokens': 164208.0, 'eval_mean_token_accuracy': 0.7294952984779112, 'epoch': 1.05}\n",
      " 35% 168/480 [03:54<05:34,  1.07s/it]\n",
      "100% 31/31 [00:02<00:00, 11.81it/s]\u001b[A\n",
      " 40% 192/480 [04:18<04:39,  1.03it/s]\n",
      "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6% 2/31 [00:00<00:01, 18.35it/s]\u001b[A\n",
      " 13% 4/31 [00:00<00:02, 12.30it/s]\u001b[A\n",
      " 19% 6/31 [00:00<00:02, 11.85it/s]\u001b[A\n",
      " 26% 8/31 [00:00<00:01, 11.87it/s]\u001b[A\n",
      " 32% 10/31 [00:00<00:01, 11.93it/s]\u001b[A\n",
      " 39% 12/31 [00:00<00:01, 11.98it/s]\u001b[A\n",
      " 45% 14/31 [00:01<00:01, 11.99it/s]\u001b[A\n",
      " 52% 16/31 [00:01<00:01, 11.95it/s]\u001b[A\n",
      " 58% 18/31 [00:01<00:01, 11.90it/s]\u001b[A\n",
      " 65% 20/31 [00:01<00:00, 11.86it/s]\u001b[A\n",
      " 71% 22/31 [00:01<00:00, 11.90it/s]\u001b[A\n",
      " 77% 24/31 [00:01<00:00, 11.98it/s]\u001b[A\n",
      " 84% 26/31 [00:02<00:00, 12.08it/s]\u001b[A\n",
      " 90% 28/31 [00:02<00:00, 12.11it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 1.5088542699813843, 'eval_runtime': 2.6601, 'eval_samples_per_second': 45.863, 'eval_steps_per_second': 11.654, 'eval_num_tokens': 187921.0, 'eval_mean_token_accuracy': 0.7247114383405254, 'epoch': 1.2}\n",
      " 40% 192/480 [04:21<04:39,  1.03it/s]\n",
      "100% 31/31 [00:02<00:00, 12.12it/s]\u001b[A\n",
      "{'loss': 0.7121, 'grad_norm': 2.5402684211730957, 'learning_rate': 0.00013570812372083998, 'num_tokens': 195681.0, 'mean_token_accuracy': 0.8314591274403109, 'epoch': 1.25}\n",
      " 45% 216/480 [05:00<04:30,  1.02s/it]\n",
      "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6% 2/31 [00:00<00:01, 18.36it/s]\u001b[A\n",
      " 13% 4/31 [00:00<00:02, 12.41it/s]\u001b[A\n",
      " 19% 6/31 [00:00<00:02, 12.06it/s]\u001b[A\n",
      " 26% 8/31 [00:00<00:01, 12.06it/s]\u001b[A\n",
      " 32% 10/31 [00:00<00:01, 12.09it/s]\u001b[A\n",
      " 39% 12/31 [00:00<00:01, 12.10it/s]\u001b[A\n",
      " 45% 14/31 [00:01<00:01, 12.08it/s]\u001b[A\n",
      " 52% 16/31 [00:01<00:01, 12.02it/s]\u001b[A\n",
      " 58% 18/31 [00:01<00:01, 12.04it/s]\u001b[A\n",
      " 65% 20/31 [00:01<00:00, 12.10it/s]\u001b[A\n",
      " 71% 22/31 [00:01<00:00, 12.06it/s]\u001b[A\n",
      " 77% 24/31 [00:01<00:00, 12.01it/s]\u001b[A\n",
      " 84% 26/31 [00:02<00:00, 11.98it/s]\u001b[A\n",
      " 90% 28/31 [00:02<00:00, 11.83it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 1.5064290761947632, 'eval_runtime': 2.6606, 'eval_samples_per_second': 45.855, 'eval_steps_per_second': 11.652, 'eval_num_tokens': 211102.0, 'eval_mean_token_accuracy': 0.7265335552154049, 'epoch': 1.35}\n",
      " 45% 216/480 [05:03<04:30,  1.02s/it]\n",
      "100% 31/31 [00:02<00:00, 11.88it/s]\u001b[A\n",
      " 50% 240/480 [05:27<04:01,  1.01s/it]\n",
      "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6% 2/31 [00:00<00:01, 18.38it/s]\u001b[A\n",
      " 13% 4/31 [00:00<00:02, 12.31it/s]\u001b[A\n",
      " 19% 6/31 [00:00<00:02, 11.56it/s]\u001b[A\n",
      " 26% 8/31 [00:00<00:01, 11.78it/s]\u001b[A\n",
      " 32% 10/31 [00:00<00:01, 11.94it/s]\u001b[A\n",
      " 39% 12/31 [00:00<00:01, 12.05it/s]\u001b[A\n",
      " 45% 14/31 [00:01<00:01, 12.13it/s]\u001b[A\n",
      " 52% 16/31 [00:01<00:01, 12.07it/s]\u001b[A\n",
      " 58% 18/31 [00:01<00:01, 12.10it/s]\u001b[A\n",
      " 65% 20/31 [00:01<00:00, 12.13it/s]\u001b[A\n",
      " 71% 22/31 [00:01<00:00, 12.15it/s]\u001b[A\n",
      " 77% 24/31 [00:01<00:00, 12.15it/s]\u001b[A\n",
      " 84% 26/31 [00:02<00:00, 12.15it/s]\u001b[A\n",
      " 90% 28/31 [00:02<00:00, 12.10it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 1.5056794881820679, 'eval_runtime': 2.6571, 'eval_samples_per_second': 45.915, 'eval_steps_per_second': 11.667, 'eval_num_tokens': 234265.0, 'eval_mean_token_accuracy': 0.7243823293716677, 'epoch': 1.5}\n",
      " 50% 240/480 [05:30<04:01,  1.01s/it]\n",
      "100% 31/31 [00:02<00:00, 12.11it/s]\u001b[A\n",
      "{'loss': 0.5761, 'grad_norm': 1.9385935068130493, 'learning_rate': 0.00010206669012275545, 'num_tokens': 244248.0, 'mean_token_accuracy': 0.8506174176931381, 'epoch': 1.56}\n",
      " 55% 264/480 [06:05<03:48,  1.06s/it]\n",
      "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6% 2/31 [00:00<00:01, 17.62it/s]\u001b[A\n",
      " 13% 4/31 [00:00<00:02, 11.95it/s]\u001b[A\n",
      " 19% 6/31 [00:00<00:02, 11.44it/s]\u001b[A\n",
      " 26% 8/31 [00:00<00:01, 11.56it/s]\u001b[A\n",
      " 32% 10/31 [00:00<00:01, 11.29it/s]\u001b[A\n",
      " 39% 12/31 [00:01<00:01, 10.83it/s]\u001b[A\n",
      " 45% 14/31 [00:01<00:01, 10.66it/s]\u001b[A\n",
      " 52% 16/31 [00:01<00:01, 10.65it/s]\u001b[A\n",
      " 58% 18/31 [00:01<00:01, 10.45it/s]\u001b[A\n",
      " 65% 20/31 [00:01<00:01, 10.76it/s]\u001b[A\n",
      " 71% 22/31 [00:01<00:00, 10.93it/s]\u001b[A\n",
      " 77% 24/31 [00:02<00:00, 10.95it/s]\u001b[A\n",
      " 84% 26/31 [00:02<00:00, 11.31it/s]\u001b[A\n",
      " 90% 28/31 [00:02<00:00, 11.46it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 1.5310434103012085, 'eval_runtime': 2.881, 'eval_samples_per_second': 42.346, 'eval_steps_per_second': 10.76, 'eval_num_tokens': 258598.0, 'eval_mean_token_accuracy': 0.7293765343004658, 'epoch': 1.65}\n",
      " 55% 264/480 [06:08<03:48,  1.06s/it]\n",
      "100% 31/31 [00:02<00:00, 11.28it/s]\u001b[A\n",
      " 60% 288/480 [06:31<03:05,  1.03it/s]\n",
      "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6% 2/31 [00:00<00:01, 18.43it/s]\u001b[A\n",
      " 13% 4/31 [00:00<00:02, 12.48it/s]\u001b[A\n",
      " 19% 6/31 [00:00<00:02, 12.02it/s]\u001b[A\n",
      " 26% 8/31 [00:00<00:01, 11.91it/s]\u001b[A\n",
      " 32% 10/31 [00:00<00:01, 11.94it/s]\u001b[A\n",
      " 39% 12/31 [00:00<00:01, 11.94it/s]\u001b[A\n",
      " 45% 14/31 [00:01<00:01, 11.95it/s]\u001b[A\n",
      " 52% 16/31 [00:01<00:01, 11.92it/s]\u001b[A\n",
      " 58% 18/31 [00:01<00:01, 11.97it/s]\u001b[A\n",
      " 65% 20/31 [00:01<00:00, 12.02it/s]\u001b[A\n",
      " 71% 22/31 [00:01<00:00, 12.07it/s]\u001b[A\n",
      " 77% 24/31 [00:01<00:00, 11.98it/s]\u001b[A\n",
      " 84% 26/31 [00:02<00:00, 12.04it/s]\u001b[A\n",
      " 90% 28/31 [00:02<00:00, 12.02it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 1.5135143995285034, 'eval_runtime': 2.6642, 'eval_samples_per_second': 45.792, 'eval_steps_per_second': 11.636, 'eval_num_tokens': 281776.0, 'eval_mean_token_accuracy': 0.7267576331092466, 'epoch': 1.8}\n",
      " 60% 288/480 [06:34<03:05,  1.03it/s]\n",
      "100% 31/31 [00:02<00:00, 11.96it/s]\u001b[A\n",
      "{'loss': 0.5645, 'grad_norm': 1.7925299406051636, 'learning_rate': 6.818243528455618e-05, 'num_tokens': 294384.0, 'mean_token_accuracy': 0.8507968002557754, 'epoch': 1.87}\n",
      " 65% 312/480 [07:13<02:52,  1.03s/it]\n",
      "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6% 2/31 [00:00<00:01, 18.39it/s]\u001b[A\n",
      " 13% 4/31 [00:00<00:02, 12.33it/s]\u001b[A\n",
      " 19% 6/31 [00:00<00:02, 11.93it/s]\u001b[A\n",
      " 26% 8/31 [00:00<00:01, 11.93it/s]\u001b[A\n",
      " 32% 10/31 [00:00<00:01, 11.92it/s]\u001b[A\n",
      " 39% 12/31 [00:00<00:01, 11.94it/s]\u001b[A\n",
      " 45% 14/31 [00:01<00:01, 11.95it/s]\u001b[A\n",
      " 52% 16/31 [00:01<00:01, 11.92it/s]\u001b[A\n",
      " 58% 18/31 [00:01<00:01, 11.95it/s]\u001b[A\n",
      " 65% 20/31 [00:01<00:00, 11.94it/s]\u001b[A\n",
      " 71% 22/31 [00:01<00:00, 11.94it/s]\u001b[A\n",
      " 77% 24/31 [00:01<00:00, 11.92it/s]\u001b[A\n",
      " 84% 26/31 [00:02<00:00, 11.91it/s]\u001b[A\n",
      " 90% 28/31 [00:02<00:00, 11.91it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 1.534711241722107, 'eval_runtime': 2.6726, 'eval_samples_per_second': 45.648, 'eval_steps_per_second': 11.599, 'eval_num_tokens': 305431.0, 'eval_mean_token_accuracy': 0.726416269617696, 'epoch': 1.95}\n",
      " 65% 312/480 [07:16<02:52,  1.03s/it]\n",
      "100% 31/31 [00:02<00:00, 11.97it/s]\u001b[A\n",
      " 70% 336/480 [07:40<02:20,  1.02it/s]\n",
      "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6% 2/31 [00:00<00:01, 18.41it/s]\u001b[A\n",
      " 13% 4/31 [00:00<00:02, 12.44it/s]\u001b[A\n",
      " 19% 6/31 [00:00<00:02, 12.10it/s]\u001b[A\n",
      " 26% 8/31 [00:00<00:01, 12.04it/s]\u001b[A\n",
      " 32% 10/31 [00:00<00:01, 12.07it/s]\u001b[A\n",
      " 39% 12/31 [00:00<00:01, 12.11it/s]\u001b[A\n",
      " 45% 14/31 [00:01<00:01, 12.15it/s]\u001b[A\n",
      " 52% 16/31 [00:01<00:01, 12.16it/s]\u001b[A\n",
      " 58% 18/31 [00:01<00:01, 12.13it/s]\u001b[A\n",
      " 65% 20/31 [00:01<00:00, 12.13it/s]\u001b[A\n",
      " 71% 22/31 [00:01<00:00, 12.08it/s]\u001b[A\n",
      " 77% 24/31 [00:01<00:00, 12.05it/s]\u001b[A\n",
      " 84% 26/31 [00:02<00:00, 12.03it/s]\u001b[A\n",
      " 90% 28/31 [00:02<00:00, 12.00it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 1.7401597499847412, 'eval_runtime': 2.6454, 'eval_samples_per_second': 46.117, 'eval_steps_per_second': 11.718, 'eval_num_tokens': 328590.0, 'eval_mean_token_accuracy': 0.7210271300808075, 'epoch': 2.1}\n",
      " 70% 336/480 [07:43<02:20,  1.02it/s]\n",
      "100% 31/31 [00:02<00:00, 12.02it/s]\u001b[A\n",
      "{'loss': 0.416, 'grad_norm': 1.7110778093338013, 'learning_rate': 3.803651568905554e-05, 'num_tokens': 341426.0, 'mean_token_accuracy': 0.8877998699646185, 'epoch': 2.19}\n",
      " 75% 360/480 [08:22<02:03,  1.03s/it]\n",
      "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6% 2/31 [00:00<00:01, 18.36it/s]\u001b[A\n",
      " 13% 4/31 [00:00<00:02, 12.37it/s]\u001b[A\n",
      " 19% 6/31 [00:00<00:02, 12.06it/s]\u001b[A\n",
      " 26% 8/31 [00:00<00:01, 12.01it/s]\u001b[A\n",
      " 32% 10/31 [00:00<00:01, 12.04it/s]\u001b[A\n",
      " 39% 12/31 [00:00<00:01, 11.84it/s]\u001b[A\n",
      " 45% 14/31 [00:01<00:01, 11.84it/s]\u001b[A\n",
      " 52% 16/31 [00:01<00:01, 11.64it/s]\u001b[A\n",
      " 58% 18/31 [00:01<00:01, 11.57it/s]\u001b[A\n",
      " 65% 20/31 [00:01<00:00, 11.68it/s]\u001b[A\n",
      " 71% 22/31 [00:01<00:00, 11.74it/s]\u001b[A\n",
      " 77% 24/31 [00:02<00:00, 11.75it/s]\u001b[A\n",
      " 84% 26/31 [00:02<00:00, 11.77it/s]\u001b[A\n",
      " 90% 28/31 [00:02<00:00, 11.79it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 1.702404260635376, 'eval_runtime': 2.7006, 'eval_samples_per_second': 45.175, 'eval_steps_per_second': 11.479, 'eval_num_tokens': 351245.0, 'eval_mean_token_accuracy': 0.7197057325993815, 'epoch': 2.25}\n",
      " 75% 360/480 [08:25<02:03,  1.03s/it]\n",
      "100% 31/31 [00:02<00:00, 11.87it/s]\u001b[A\n",
      " 80% 384/480 [08:49<01:34,  1.02it/s]\n",
      "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6% 2/31 [00:00<00:01, 18.37it/s]\u001b[A\n",
      " 13% 4/31 [00:00<00:02, 12.41it/s]\u001b[A\n",
      " 19% 6/31 [00:00<00:02, 12.04it/s]\u001b[A\n",
      " 26% 8/31 [00:00<00:01, 12.05it/s]\u001b[A\n",
      " 32% 10/31 [00:00<00:01, 12.09it/s]\u001b[A\n",
      " 39% 12/31 [00:00<00:01, 12.10it/s]\u001b[A\n",
      " 45% 14/31 [00:01<00:01, 12.08it/s]\u001b[A\n",
      " 52% 16/31 [00:01<00:01, 12.02it/s]\u001b[A\n",
      " 58% 18/31 [00:01<00:01, 12.02it/s]\u001b[A\n",
      " 65% 20/31 [00:01<00:00, 12.04it/s]\u001b[A\n",
      " 71% 22/31 [00:01<00:00, 12.09it/s]\u001b[A\n",
      " 77% 24/31 [00:01<00:00, 12.03it/s]\u001b[A\n",
      " 84% 26/31 [00:02<00:00, 12.04it/s]\u001b[A\n",
      " 90% 28/31 [00:02<00:00, 12.05it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 1.7217525243759155, 'eval_runtime': 2.6461, 'eval_samples_per_second': 46.106, 'eval_steps_per_second': 11.715, 'eval_num_tokens': 375147.0, 'eval_mean_token_accuracy': 0.7208595564288478, 'epoch': 2.4}\n",
      " 80% 384/480 [08:52<01:34,  1.02it/s]\n",
      "100% 31/31 [00:02<00:00, 12.10it/s]\u001b[A\n",
      "{'loss': 0.2963, 'grad_norm': 1.7152810096740723, 'learning_rate': 1.5170860288242638e-05, 'num_tokens': 390458.0, 'mean_token_accuracy': 0.9126129478216172, 'epoch': 2.5}\n",
      " 85% 408/480 [09:34<01:12,  1.01s/it]\n",
      "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6% 2/31 [00:00<00:01, 18.39it/s]\u001b[A\n",
      " 13% 4/31 [00:00<00:02, 12.24it/s]\u001b[A\n",
      " 19% 6/31 [00:00<00:02, 11.57it/s]\u001b[A\n",
      " 26% 8/31 [00:00<00:02, 11.35it/s]\u001b[A\n",
      " 32% 10/31 [00:00<00:01, 11.33it/s]\u001b[A\n",
      " 39% 12/31 [00:01<00:01, 11.03it/s]\u001b[A\n",
      " 45% 14/31 [00:01<00:01, 11.20it/s]\u001b[A\n",
      " 52% 16/31 [00:01<00:01, 11.11it/s]\u001b[A\n",
      " 58% 18/31 [00:01<00:01, 11.05it/s]\u001b[A\n",
      " 65% 20/31 [00:01<00:01, 11.00it/s]\u001b[A\n",
      " 71% 22/31 [00:01<00:00, 10.91it/s]\u001b[A\n",
      " 77% 24/31 [00:02<00:00, 10.95it/s]\u001b[A\n",
      " 84% 26/31 [00:02<00:00, 10.79it/s]\u001b[A\n",
      " 90% 28/31 [00:02<00:00, 10.87it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 1.70298171043396, 'eval_runtime': 2.9008, 'eval_samples_per_second': 42.057, 'eval_steps_per_second': 10.687, 'eval_num_tokens': 398339.0, 'eval_mean_token_accuracy': 0.7210498009958575, 'epoch': 2.55}\n",
      " 85% 408/480 [09:37<01:12,  1.01s/it]\n",
      "100% 31/31 [00:02<00:00, 10.73it/s]\u001b[A\n",
      " 90% 432/480 [10:01<00:46,  1.03it/s]\n",
      "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6% 2/31 [00:00<00:01, 18.38it/s]\u001b[A\n",
      " 13% 4/31 [00:00<00:02, 12.40it/s]\u001b[A\n",
      " 19% 6/31 [00:00<00:02, 12.03it/s]\u001b[A\n",
      " 26% 8/31 [00:00<00:01, 11.91it/s]\u001b[A\n",
      " 32% 10/31 [00:00<00:01, 11.87it/s]\u001b[A\n",
      " 39% 12/31 [00:00<00:01, 11.93it/s]\u001b[A\n",
      " 45% 14/31 [00:01<00:01, 11.99it/s]\u001b[A\n",
      " 52% 16/31 [00:01<00:01, 12.03it/s]\u001b[A\n",
      " 58% 18/31 [00:01<00:01, 12.04it/s]\u001b[A\n",
      " 65% 20/31 [00:01<00:00, 12.09it/s]\u001b[A\n",
      " 71% 22/31 [00:01<00:00, 12.12it/s]\u001b[A\n",
      " 77% 24/31 [00:01<00:00, 12.15it/s]\u001b[A\n",
      " 84% 26/31 [00:02<00:00, 12.16it/s]\u001b[A\n",
      " 90% 28/31 [00:02<00:00, 12.17it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 1.715834379196167, 'eval_runtime': 2.6429, 'eval_samples_per_second': 46.162, 'eval_steps_per_second': 11.73, 'eval_num_tokens': 421670.0, 'eval_mean_token_accuracy': 0.718869803413268, 'epoch': 2.7}\n",
      " 90% 432/480 [10:03<00:46,  1.03it/s]\n",
      "100% 31/31 [00:02<00:00, 12.18it/s]\u001b[A\n",
      "{'loss': 0.2914, 'grad_norm': 1.9852969646453857, 'learning_rate': 2.2720193004240774e-06, 'num_tokens': 439613.0, 'mean_token_accuracy': 0.912337831556797, 'epoch': 2.81}\n",
      " 95% 456/480 [10:45<00:25,  1.06s/it]\n",
      "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6% 2/31 [00:00<00:01, 18.38it/s]\u001b[A\n",
      " 13% 4/31 [00:00<00:02, 12.35it/s]\u001b[A\n",
      " 19% 6/31 [00:00<00:02, 12.09it/s]\u001b[A\n",
      " 26% 8/31 [00:00<00:01, 12.10it/s]\u001b[A\n",
      " 32% 10/31 [00:00<00:01, 12.02it/s]\u001b[A\n",
      " 39% 12/31 [00:00<00:01, 11.96it/s]\u001b[A\n",
      " 45% 14/31 [00:01<00:01, 11.97it/s]\u001b[A\n",
      " 52% 16/31 [00:01<00:01, 11.91it/s]\u001b[A\n",
      " 58% 18/31 [00:01<00:01, 11.75it/s]\u001b[A\n",
      " 65% 20/31 [00:01<00:00, 11.72it/s]\u001b[A\n",
      " 71% 22/31 [00:01<00:00, 11.66it/s]\u001b[A\n",
      " 77% 24/31 [00:02<00:00, 11.73it/s]\u001b[A\n",
      " 84% 26/31 [00:02<00:00, 11.76it/s]\u001b[A\n",
      " 90% 28/31 [00:02<00:00, 11.84it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 1.7203152179718018, 'eval_runtime': 2.6871, 'eval_samples_per_second': 45.403, 'eval_steps_per_second': 11.537, 'eval_num_tokens': 445092.0, 'eval_mean_token_accuracy': 0.719962913182474, 'epoch': 2.85}\n",
      " 95% 456/480 [10:47<00:25,  1.06s/it]\n",
      "100% 31/31 [00:02<00:00, 11.96it/s]\u001b[A\n",
      "100% 480/480 [11:12<00:00,  1.00s/it]\n",
      "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  6% 2/31 [00:00<00:01, 18.41it/s]\u001b[A\n",
      " 13% 4/31 [00:00<00:02, 12.46it/s]\u001b[A\n",
      " 19% 6/31 [00:00<00:02, 11.98it/s]\u001b[A\n",
      " 26% 8/31 [00:00<00:01, 12.04it/s]\u001b[A\n",
      " 32% 10/31 [00:00<00:01, 12.08it/s]\u001b[A\n",
      " 39% 12/31 [00:00<00:01, 11.87it/s]\u001b[A\n",
      " 45% 14/31 [00:01<00:01, 11.82it/s]\u001b[A\n",
      " 52% 16/31 [00:01<00:01, 11.79it/s]\u001b[A\n",
      " 58% 18/31 [00:01<00:01, 11.78it/s]\u001b[A\n",
      " 65% 20/31 [00:01<00:00, 11.79it/s]\u001b[A\n",
      " 71% 22/31 [00:01<00:00, 11.86it/s]\u001b[A\n",
      " 77% 24/31 [00:01<00:00, 11.89it/s]\u001b[A\n",
      " 84% 26/31 [00:02<00:00, 11.99it/s]\u001b[A\n",
      " 90% 28/31 [00:02<00:00, 12.03it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 1.720878005027771, 'eval_runtime': 2.6774, 'eval_samples_per_second': 45.566, 'eval_steps_per_second': 11.578, 'eval_num_tokens': 469229.0, 'eval_mean_token_accuracy': 0.7201051385171952, 'epoch': 3.0}\n",
      "100% 480/480 [11:14<00:00,  1.00s/it]\n",
      "100% 31/31 [00:02<00:00, 11.93it/s]\u001b[A\n",
      "{'train_runtime': 714.2846, 'train_samples_per_second': 10.794, 'train_steps_per_second': 0.672, 'train_loss': 0.8054648876190186, 'num_tokens': 469229.0, 'mean_token_accuracy': 0.9177080204089483, 'epoch': 3.0}\n",
      "100% 480/480 [11:54<00:00,  1.49s/it]\n",
      "Saving adapter to deepseek-paraphrase-lora/final\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training script used TRL's SFTTrainer with the following configuration (subset check train.py for more info):\n",
    "\n",
    "- LoRA rank: 16\n",
    "- LoRA alpha: 32\n",
    "- Learning rate: 2e-4\n",
    "- Epochs: 3\n",
    "- Scheduler: Cosine with warmup\n",
    "- Target modules: Attention (q_proj, k_proj, v_proj, o_proj) and MLP layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Evaluation Metrics\n",
    "\n",
    "Our evaluation system uses the following metrics to comprehensively assess paraphrase quality:\n",
    "\n",
    "### 1. BERTScore\n",
    "**Mathematical definition:** \n",
    "$$\\text{BERTScore} = F_1(\\text{P}, \\text{R})$$\n",
    "where:\n",
    "- $\\text{P} = \\max_{j} \\cos(\\mathbf{x}_i, \\mathbf{y}_j)$ (Precision)\n",
    "- $\\text{R} = \\max_{i} \\cos(\\mathbf{x}_i, \\mathbf{y}_j)$ (Recall)\n",
    "- $\\mathbf{x}_i, \\mathbf{y}_j$ are contextualized embeddings of tokens\n",
    "\n",
    "**Intuition:** BERTScore uses BERT embeddings to match words in the original and paraphrase texts. It aligns each word with its most similar counterpart in the other text, measuring cosine similarity between embeddings. This captures semantic similarity even when different words are used. Higher values (closer to 1) indicate better meaning preservation.\n",
    "\n",
    "### 2. BLEU Diversity\n",
    "**Mathematical definition:** \n",
    "$$\\text{BLEU\\_Diversity} = 1 - \\text{BLEU}(original, paraphrase)$$\n",
    "\n",
    "where BLEU is calculated as:\n",
    "$$\\text{BLEU} = BP \\cdot \\exp\\left(\\sum_{n=1}^{N} w_n \\log p_n\\right)$$\n",
    "- $p_n$ = n-gram precision\n",
    "- $BP$ = brevity penalty\n",
    "- $w_n$ = n-gram weights (we use $w_1=0.4, w_2=0.3, w_3=0.2, w_4=0.1$)\n",
    "\n",
    "**Intuition:** BLEU measures n-gram overlap between texts, with a penalty for shorter outputs. By inverting it (1-BLEU), we measure lexical diversity. Higher BLEU_Diversity values indicate more diverse word choices and phrasing.\n",
    "\n",
    "### 3. Edit Distance\n",
    "**Mathematical definition:** \n",
    "$$\\text{Edit\\_Distance} = \\frac{\\text{Levenshtein}(original, paraphrase)}{\\max(\\text{len}(original), \\text{len}(paraphrase))}$$\n",
    "\n",
    "where Levenshtein distance counts the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into another:\n",
    "\n",
    "$$Lev_{a,b}(i,j) = \\begin{cases}\n",
    "\\max(i,j) & \\text{if } \\min(i,j) = 0 \\\\\n",
    "\\min \\begin{cases}\n",
    "Lev_{a,b}(i-1,j) + 1 \\\\\n",
    "Lev_{a,b}(i,j-1) + 1 \\\\\n",
    "Lev_{a,b}(i-1,j-1) + [a_i \\neq b_j]\n",
    "\\end{cases} & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Intuition:** Edit distance captures character-level changes needed to transform the original text into the paraphrase. The normalized version divides by the length of the longer text to account for text length. Higher values indicate more substantial modifications.\n",
    "\n",
    "### 4. Syntactic Diversity\n",
    "**Mathematical definition:** \n",
    "$$\\text{Syntactic\\_Diversity} = 1 - \\frac{|D_1 \\cap D_2|}{\\max(|D_1|, |D_2|)}$$\n",
    "\n",
    "where:\n",
    "- $D_1 = \\{dep_1, dep_2, ..., dep_n\\}$ is the set of dependency relations in the original\n",
    "- $D_2 = \\{dep_1, dep_2, ..., dep_m\\}$ is the set of dependency relations in the paraphrase\n",
    "\n",
    "**Intuition:** This measures structural differences in sentences by analyzing dependency parsing patterns (subject-verb-object relationships, etc.). Higher values indicate that the sentence structure has been significantly changed while maintaining meaning. \n",
    "\n",
    "### Syntactic Diversity Example\n",
    "\n",
    "#### Original Sentence:\n",
    "> \"The professor explained the concept to the students.\"\n",
    "\n",
    "```\n",
    "Dependencies: [\"nsubj\", \"ROOT\", \"det\", \"dobj\", \"prep\", \"pobj\"]\n",
    "```\n",
    "\n",
    "#### Example 1: Low Syntactic Diversity\n",
    "\n",
    "> \"The teacher explained the idea to the pupils.\"\n",
    "\n",
    "```\n",
    "Dependencies: [\"nsubj\", \"ROOT\", \"det\", \"dobj\", \"prep\", \"pobj\"]\n",
    "```\n",
    "\n",
    "**Analysis:**\n",
    "- Same dependency structure, different words\n",
    "- Overlap: 6/6 = 1.0\n",
    "- Syntactic Diversity: 1 - 1.0 = **0.0** (very low)\n",
    "\n",
    "```\n",
    "Original:  [nsubj]→[ROOT]→[det]→[dobj]→[prep]→[pobj]\n",
    "           ↓       ↓       ↓      ↓      ↓      ↓\n",
    "Paraphrase:[nsubj]→[ROOT]→[det]→[dobj]→[prep]→[pobj]\n",
    "```\n",
    "\n",
    "#### Example 2: High Syntactic Diversity\n",
    "\n",
    "> \"The concept was explained to students by the professor.\"\n",
    "\n",
    "```\n",
    "Dependencies: [\"det\", \"nsubjpass\", \"auxpass\", \"ROOT\", \"prep\", \"pobj\", \"agent\", \"pobj\"]\n",
    "```\n",
    "\n",
    "**Analysis:**\n",
    "- Different structure (passive voice)\n",
    "- Common dependencies: [\"det\", \"ROOT\", \"prep\", \"pobj\"]\n",
    "- Overlap: 4/8 = 0.5\n",
    "- Syntactic Diversity: 1 - 0.5 = **0.5** (much higher)\n",
    "\n",
    "```\n",
    "Original:  [nsubj]→[ROOT]→[det]→[dobj]→[prep]→[pobj]\n",
    "           ↓       ↓       ↓      ↓      ↓      ↓\n",
    "Paraphrase:[det]→[nsubjpass]→[auxpass]→[ROOT]→[prep]→[pobj]→[agent]→[pobj]\n",
    "            ↑                          ↑       ↑      ↑               ↑ \n",
    "            |__________________________|_______|______________________|\n",
    "                                 Matching dependencies\n",
    "```\n",
    "\n",
    "### 5. Harmonic Score\n",
    "**Mathematical definition:** \n",
    "$$\\text{Harmonic\\_Score} = \\frac{2 \\times \\text{BERTScore} \\times \\text{Avg\\_Diversity}}{\\text{BERTScore} + \\text{Avg\\_Diversity}}$$\n",
    "\n",
    "where:\n",
    "$$\\text{Avg\\_Diversity} = \\frac{\\text{BLEU\\_Diversity} + \\text{Syntactic\\_Diversity}}{2}$$\n",
    "\n",
    "**Intuition:** The harmonic mean balances semantic preservation (BERTScore) with diversity measures. It ensures that a good paraphrase both preserves the original meaning and uses diverse wording/structure. A simple average would allow one metric to compensate for a poor score in the other, while the harmonic mean requires both to be high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Model Evaluation\n",
    "\n",
    "We evaluated three models:\n",
    "1. **BART** (eugenesiow/bart-paraphrase) - Baseline model\n",
    "2. **T5** (mrm8488/t5-small-finetuned-quora-for-paraphrasing) - Baseline model\n",
    "3. **DeepSeek-LoRA** - Our fine-tuned model (based on deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)\n",
    "\n",
    "Using a single evaluation script where we changed the model name:\n",
    "\n",
    "!python evaluate.py --model MODEL_NAME --dataset data/evaluation/collected_sentences.json --output data/evaluation/results_MODEL_NAME.json --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompting strategy for each model during evaluation:\n",
      "\n",
      "T5 (Seq2Seq): \"paraphrase: [input text]\"\n",
      "\n",
      "BART (Seq2Seq): \"paraphrase: [input text]\"\n",
      "\n",
      "DeepSeek-LoRA (Autoregressive):\n",
      "<｜begin▁of▁sentence｜><｜User｜>Paraphrase the following text while preserving its meaning but changing the wording and structure: [input text]<｜Assistant｜><think>\n",
      "Let me analyze this text and find ways to rephrase it while keeping the same meaning.\n",
      "I need to use different vocabulary and structure.\n",
      "</think>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Prompting strategy for each model during evaluation:\")\n",
    "print(\"\"\"\n",
    "T5 (Seq2Seq): \"paraphrase: [input text]\"\n",
    "\n",
    "BART (Seq2Seq): \"paraphrase: [input text]\"\n",
    "\n",
    "DeepSeek-LoRA (Autoregressive):\n",
    "<｜begin▁of▁sentence｜><｜User｜>Paraphrase the following text while preserving its meaning but changing the wording and structure: [input text]<｜Assistant｜><think>\n",
    "Let me analyze this text and find ways to rephrase it while keeping the same meaning.\n",
    "I need to use different vocabulary and structure.\n",
    "</think>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example the deepseek model eval is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 909731,
     "status": "ok",
     "timestamp": 1747812103378,
     "user": {
      "displayName": "Peter A. Massih",
      "userId": "02579015314160660239"
     },
     "user_tz": 240
    },
    "id": "WN1tGwOLb-Vi",
    "outputId": "2971d59c-7ded-4de9-9f18-78f9851fb77f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-21 07:06:40.640860: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747811200.662775   24130 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747811200.669408   24130 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Hugging Face token not found. Please enter your token (input will be hidden):\n",
      "Password: \n",
      "Successfully logged in to Hugging Face\n",
      "Successfully loaded dataset with 212 items from 4 categories\n",
      "Loading model deepseek-paraphrase-lora/final...\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100% 2/2 [00:03<00:00,  1.94s/it]\n",
      "Loaded as a Causal LM model\n",
      "Model loaded successfully\n",
      "Evaluating deepseek-paraphrase-lora/final:   0% 0/212 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:   0% 1/212 [00:04<16:44,  4.76s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:   1% 2/212 [00:07<12:31,  3.58s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:   1% 3/212 [00:12<14:11,  4.07s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:   2% 4/212 [00:15<12:47,  3.69s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:   2% 5/212 [01:10<1:16:51, 22.28s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:   3% 6/212 [01:13<54:29, 15.87s/it]  Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:   3% 7/212 [01:17<40:45, 11.93s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:   4% 8/212 [01:25<36:07, 10.63s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:   4% 9/212 [01:30<29:32,  8.73s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:   5% 10/212 [01:34<25:04,  7.45s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:   5% 11/212 [01:38<21:19,  6.37s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:   6% 12/212 [01:41<18:02,  5.41s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:   6% 13/212 [01:45<15:43,  4.74s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:   7% 14/212 [01:47<13:41,  4.15s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:   7% 15/212 [01:52<14:09,  4.31s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:   8% 16/212 [01:55<12:42,  3.89s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:   8% 17/212 [01:58<11:46,  3.62s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:   8% 18/212 [02:01<10:49,  3.35s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:   9% 19/212 [02:05<11:17,  3.51s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:   9% 20/212 [02:08<11:27,  3.58s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  10% 21/212 [02:12<11:28,  3.60s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  10% 22/212 [02:15<10:43,  3.38s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  11% 23/212 [02:18<10:12,  3.24s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  11% 24/212 [02:22<11:28,  3.66s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  12% 25/212 [02:26<11:36,  3.72s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  12% 26/212 [02:29<10:37,  3.43s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  13% 27/212 [02:32<10:18,  3.35s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  13% 28/212 [02:34<09:15,  3.02s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  14% 29/212 [02:38<09:27,  3.10s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  14% 30/212 [02:44<12:00,  3.96s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  15% 31/212 [02:46<10:35,  3.51s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  15% 32/212 [02:49<10:06,  3.37s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  16% 33/212 [02:52<09:46,  3.28s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  16% 34/212 [02:55<09:20,  3.15s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  17% 35/212 [02:59<09:36,  3.26s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  17% 36/212 [03:02<09:53,  3.37s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  17% 37/212 [03:05<09:10,  3.15s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  18% 38/212 [03:08<09:20,  3.22s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  18% 39/212 [03:12<09:56,  3.45s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  19% 40/212 [03:15<09:41,  3.38s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  19% 41/212 [03:20<10:14,  3.59s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  20% 42/212 [03:26<12:50,  4.53s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  20% 43/212 [03:30<11:49,  4.20s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  21% 44/212 [03:34<12:08,  4.34s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  21% 45/212 [03:40<12:55,  4.64s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  22% 46/212 [03:43<11:37,  4.20s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  22% 47/212 [03:45<10:16,  3.74s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  23% 48/212 [03:50<10:27,  3.83s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  23% 49/212 [03:54<10:59,  4.05s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  24% 50/212 [03:58<10:56,  4.05s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  24% 51/212 [04:02<10:31,  3.92s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  25% 52/212 [04:06<11:02,  4.14s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  25% 53/212 [04:09<09:58,  3.76s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  25% 54/212 [04:14<10:25,  3.96s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  26% 55/212 [04:17<10:08,  3.87s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  26% 56/212 [04:20<09:00,  3.47s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  27% 57/212 [04:24<09:02,  3.50s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  27% 58/212 [04:25<07:29,  2.92s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  28% 59/212 [04:29<08:35,  3.37s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  28% 60/212 [04:33<08:49,  3.48s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  29% 61/212 [04:37<08:43,  3.46s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  29% 62/212 [04:41<08:57,  3.59s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  30% 63/212 [04:44<08:31,  3.44s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  30% 64/212 [04:47<08:38,  3.50s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  31% 65/212 [04:50<08:20,  3.41s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  31% 66/212 [04:55<09:06,  3.74s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  32% 67/212 [04:59<09:27,  3.91s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  32% 68/212 [05:02<08:48,  3.67s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  33% 69/212 [05:07<09:46,  4.10s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  33% 70/212 [05:11<09:26,  3.99s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  33% 71/212 [05:18<11:05,  4.72s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  34% 72/212 [05:21<10:20,  4.43s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  34% 73/212 [05:24<09:19,  4.02s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  35% 74/212 [05:28<08:58,  3.90s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  35% 75/212 [05:34<09:58,  4.37s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  36% 76/212 [05:37<08:57,  3.95s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  36% 77/212 [05:40<08:29,  3.78s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  37% 78/212 [05:44<08:20,  3.73s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  37% 79/212 [05:48<08:31,  3.84s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  38% 80/212 [05:52<08:51,  4.03s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  38% 81/212 [05:55<08:14,  3.78s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  39% 82/212 [05:59<08:09,  3.76s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  39% 83/212 [06:04<09:10,  4.26s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  40% 84/212 [06:08<08:19,  3.90s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  40% 85/212 [06:10<07:40,  3.63s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  41% 86/212 [06:14<07:15,  3.46s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  41% 87/212 [06:16<06:51,  3.29s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  42% 88/212 [06:20<07:03,  3.42s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  42% 89/212 [06:24<07:09,  3.49s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  42% 90/212 [06:28<07:39,  3.77s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  43% 91/212 [06:31<07:06,  3.52s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  43% 92/212 [06:34<06:35,  3.30s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  44% 93/212 [06:37<06:36,  3.33s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  44% 94/212 [06:41<06:42,  3.41s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  45% 95/212 [06:44<06:24,  3.28s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  45% 96/212 [06:47<06:09,  3.19s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  46% 97/212 [06:51<06:52,  3.59s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  46% 98/212 [06:55<06:48,  3.58s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  47% 99/212 [06:58<06:35,  3.50s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  47% 100/212 [07:02<06:31,  3.49s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  48% 101/212 [07:05<06:01,  3.26s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  48% 102/212 [07:09<06:48,  3.71s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  49% 103/212 [07:12<06:07,  3.38s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  49% 104/212 [07:15<06:00,  3.34s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  50% 105/212 [07:18<05:37,  3.15s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  50% 106/212 [07:21<05:35,  3.17s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  50% 107/212 [07:26<06:27,  3.69s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  51% 108/212 [07:30<06:29,  3.74s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  51% 109/212 [07:33<05:58,  3.48s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  52% 110/212 [07:36<05:57,  3.50s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  52% 111/212 [07:41<06:46,  4.03s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  53% 112/212 [07:46<07:11,  4.31s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  53% 113/212 [07:51<07:00,  4.25s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  54% 114/212 [07:53<06:04,  3.72s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  54% 115/212 [07:58<06:33,  4.06s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  55% 116/212 [08:01<06:08,  3.83s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  55% 117/212 [08:07<06:47,  4.29s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  56% 118/212 [08:11<06:33,  4.18s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  56% 119/212 [08:14<06:08,  3.96s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  57% 120/212 [08:17<05:42,  3.72s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  57% 121/212 [08:21<05:49,  3.84s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  58% 122/212 [08:24<05:24,  3.60s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  58% 123/212 [08:28<05:22,  3.62s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  58% 124/212 [08:32<05:41,  3.88s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  59% 125/212 [08:41<07:40,  5.29s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  59% 126/212 [08:44<06:41,  4.67s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  60% 127/212 [08:47<05:46,  4.07s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  60% 128/212 [08:51<05:42,  4.07s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  61% 129/212 [08:54<05:12,  3.77s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  61% 130/212 [09:00<06:12,  4.54s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  62% 131/212 [09:04<05:52,  4.36s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  62% 132/212 [09:08<05:31,  4.15s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  63% 133/212 [09:12<05:27,  4.15s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  63% 134/212 [09:15<05:01,  3.87s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  64% 135/212 [09:18<04:38,  3.61s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  64% 136/212 [09:22<04:42,  3.72s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  65% 137/212 [09:25<04:21,  3.49s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  65% 138/212 [09:29<04:14,  3.44s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  66% 139/212 [09:33<04:31,  3.72s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  66% 140/212 [09:37<04:34,  3.81s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  67% 141/212 [09:41<04:28,  3.79s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  67% 142/212 [09:45<04:44,  4.06s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  67% 143/212 [09:50<04:52,  4.24s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  68% 144/212 [09:59<06:16,  5.54s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  68% 145/212 [10:02<05:28,  4.91s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  69% 146/212 [10:07<05:17,  4.81s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  69% 147/212 [10:11<05:06,  4.71s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  70% 148/212 [10:15<04:38,  4.35s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  70% 149/212 [10:18<04:20,  4.13s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  71% 150/212 [10:23<04:18,  4.18s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  71% 151/212 [10:26<04:06,  4.04s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  72% 152/212 [10:30<03:49,  3.82s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  72% 153/212 [10:34<04:00,  4.08s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  73% 154/212 [10:37<03:32,  3.66s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  73% 155/212 [10:40<03:21,  3.53s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  74% 156/212 [10:43<03:13,  3.45s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  74% 157/212 [10:48<03:23,  3.71s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  75% 158/212 [10:53<03:38,  4.04s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  75% 159/212 [10:57<03:33,  4.04s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  75% 160/212 [11:00<03:12,  3.71s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  76% 161/212 [11:03<03:00,  3.53s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  76% 162/212 [11:06<02:50,  3.41s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  77% 163/212 [11:10<02:56,  3.61s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  77% 164/212 [11:13<02:43,  3.42s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  78% 165/212 [11:20<03:38,  4.65s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  78% 166/212 [11:23<03:10,  4.14s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  79% 167/212 [11:27<02:55,  3.91s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  79% 168/212 [11:30<02:40,  3.66s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  80% 169/212 [11:33<02:30,  3.49s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  80% 170/212 [11:36<02:28,  3.53s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  81% 171/212 [11:41<02:39,  3.89s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  81% 172/212 [11:45<02:28,  3.72s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  82% 173/212 [11:51<03:01,  4.64s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  82% 174/212 [11:55<02:46,  4.39s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  83% 175/212 [12:01<03:03,  4.95s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  83% 176/212 [12:06<02:53,  4.83s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  83% 177/212 [12:09<02:31,  4.32s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  84% 178/212 [12:13<02:23,  4.22s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  84% 179/212 [12:16<02:08,  3.88s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  85% 180/212 [12:20<01:59,  3.74s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  85% 181/212 [12:24<02:03,  3.99s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  86% 182/212 [12:27<01:47,  3.57s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  86% 183/212 [12:31<01:53,  3.92s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  87% 184/212 [12:37<02:06,  4.53s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  87% 185/212 [12:41<01:51,  4.14s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  88% 186/212 [12:44<01:39,  3.83s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  88% 187/212 [12:46<01:26,  3.46s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  89% 188/212 [12:49<01:20,  3.33s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  89% 189/212 [12:53<01:18,  3.43s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  90% 190/212 [12:56<01:14,  3.39s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  90% 191/212 [13:00<01:12,  3.43s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  91% 192/212 [13:03<01:09,  3.48s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  91% 193/212 [13:06<01:02,  3.26s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  92% 194/212 [13:10<01:01,  3.41s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  92% 195/212 [13:14<00:59,  3.49s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  92% 196/212 [13:18<00:58,  3.63s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  93% 197/212 [13:21<00:52,  3.51s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  93% 198/212 [13:24<00:48,  3.43s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  94% 199/212 [13:27<00:42,  3.29s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  94% 200/212 [13:30<00:39,  3.26s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  95% 201/212 [13:33<00:35,  3.25s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  95% 202/212 [13:37<00:33,  3.37s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  96% 203/212 [13:40<00:29,  3.24s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  96% 204/212 [13:48<00:36,  4.59s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  97% 205/212 [13:51<00:30,  4.32s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  97% 206/212 [13:55<00:24,  4.09s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  98% 207/212 [13:58<00:18,  3.73s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  98% 208/212 [14:01<00:14,  3.60s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  99% 209/212 [14:06<00:11,  3.99s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final:  99% 210/212 [14:10<00:07,  3.84s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final: 100% 211/212 [14:13<00:03,  3.80s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating deepseek-paraphrase-lora/final: 100% 212/212 [14:17<00:00,  4.04s/it]\n",
      "Results saved to data/evaluation/results_deepseek.json\n",
      "\n",
      "Results for deepseek-paraphrase-lora/final:\n",
      "======================================================================\n",
      "Metric               Score      Interpretation\n",
      "----------------------------------------------------------------------\n",
      "bertscore            0.9521     Excellent Semantic similarity\n",
      "bleu_diversity       0.5135     Acceptable Word choice diversity\n",
      "edit_distance        0.3439     Acceptable Character-level changes\n",
      "syntactic_diversity  0.1472     Poor Structural changes\n",
      "harmonic_score       0.4684     Acceptable Balance of meaning & diversity\n",
      "\n",
      "Results by category:\n",
      "\n",
      "ACADEMIC Category:\n",
      "----------------------------------------------------------------------\n",
      "bertscore            0.9613     Excellent Semantic similarity\n",
      "bleu_diversity       0.4769     Acceptable Word choice diversity\n",
      "edit_distance        0.3267     Acceptable Character-level changes\n",
      "syntactic_diversity  0.1534     Poor Structural changes\n",
      "harmonic_score       0.4603     Acceptable Balance of meaning & diversity\n",
      "\n",
      "ARTICLE Category:\n",
      "----------------------------------------------------------------------\n",
      "bertscore            0.9646     Excellent Semantic similarity\n",
      "bleu_diversity       0.4573     Acceptable Word choice diversity\n",
      "edit_distance        0.2850     Poor Character-level changes\n",
      "syntactic_diversity  0.1170     Poor Structural changes\n",
      "harmonic_score       0.4187     Poor Balance of meaning & diversity\n",
      "\n",
      "LITERARY Category:\n",
      "----------------------------------------------------------------------\n",
      "bertscore            0.9450     Excellent Semantic similarity\n",
      "bleu_diversity       0.5455     Acceptable Word choice diversity\n",
      "edit_distance        0.3670     Acceptable Character-level changes\n",
      "syntactic_diversity  0.1521     Poor Structural changes\n",
      "harmonic_score       0.4851     Acceptable Balance of meaning & diversity\n",
      "\n",
      "TECHNICAL Category:\n",
      "----------------------------------------------------------------------\n",
      "bertscore            0.9635     Excellent Semantic similarity\n",
      "bleu_diversity       0.3445     Poor Word choice diversity\n",
      "edit_distance        0.2201     Poor Character-level changes\n",
      "syntactic_diversity  0.0901     Poor Structural changes\n",
      "harmonic_score       0.3513     Poor Balance of meaning & diversity\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py --model deepseek-paraphrase-lora/final --dataset data/evaluation/collected_sentences.json --output data/evaluation/results_deepseek.json  --device cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bertscore</th>\n",
       "      <th>bleu_diversity</th>\n",
       "      <th>edit_distance</th>\n",
       "      <th>syntactic_diversity</th>\n",
       "      <th>harmonic_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BART</th>\n",
       "      <td>0.958706</td>\n",
       "      <td>0.234672</td>\n",
       "      <td>0.172934</td>\n",
       "      <td>0.095302</td>\n",
       "      <td>0.255030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T5</th>\n",
       "      <td>0.995169</td>\n",
       "      <td>0.041585</td>\n",
       "      <td>0.021223</td>\n",
       "      <td>0.012450</td>\n",
       "      <td>0.045644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepSeek-LoRA</th>\n",
       "      <td>0.952133</td>\n",
       "      <td>0.513501</td>\n",
       "      <td>0.343940</td>\n",
       "      <td>0.147242</td>\n",
       "      <td>0.468434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               bertscore  bleu_diversity  edit_distance  syntactic_diversity  \\\n",
       "BART            0.958706        0.234672       0.172934             0.095302   \n",
       "T5              0.995169        0.041585       0.021223             0.012450   \n",
       "DeepSeek-LoRA   0.952133        0.513501       0.343940             0.147242   \n",
       "\n",
       "               harmonic_score  \n",
       "BART                 0.255030  \n",
       "T5                   0.045644  \n",
       "DeepSeek-LoRA        0.468434  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"data/evaluation/results_bart.json\", \"r\") as f:\n",
    "    bart_results = json.load(f)\n",
    "\n",
    "with open(\"data/evaluation/results_t5.json\", \"r\") as f:\n",
    "    t5_results = json.load(f)\n",
    "    \n",
    "with open(\"data/evaluation/results_deepseek.json\", \"r\") as f:\n",
    "    deepseek_results = json.load(f)\n",
    "\n",
    "models = [\"BART\", \"T5\", \"DeepSeek-LoRA\"]\n",
    "metrics = [\"bertscore\", \"bleu_diversity\", \"edit_distance\", \"syntactic_diversity\", \"harmonic_score\"]\n",
    "\n",
    "# Extract overall metrics\n",
    "comparison_data = {}\n",
    "comparison_data[\"BART\"] = bart_results[\"aggregate\"][\"avg_metrics\"]\n",
    "comparison_data[\"T5\"] = t5_results[\"aggregate\"][\"avg_metrics\"]\n",
    "comparison_data[\"DeepSeek-LoRA\"] = deepseek_results[\"aggregate\"][\"avg_metrics\"]\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data).T\n",
    "display(comparison_df[metrics])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "comparison_df[metrics].plot(kind='bar')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Model')\n",
    "plt.legend(title='Metrics')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: LITERARY\n",
      "Original: Day after day passed away without bringing any other tidings of him than the report which shortly prevailed in Meryton of his coming no more to Netherfield the whole winter; a report which highly incensed Mrs. Bennet, and which she never failed to contradict as a most scandalous falsehood.\n",
      "\n",
      "Paraphrases:\n",
      "BART: Paraphrase: Day after day passed away without bringing any other news of him than the report which shortly prevailed in Meryton of his coming no more to Netherfield for the whole winter, a report which highly incensed Mrs. Bennet and which she never failed to contradict.\n",
      "T5: Day after day passed away without bringing any other tidings of him than the report which shortly prevailed in Meryton of his coming no more to Netherfield the whole winter; a report which highly incensed Mrs. Bennet, and which she never failed to contradict as a scandalous falsehood.\n",
      "DeepSeek-LoRA: Day after day passed without any news of him other than the report in Meryton that he would not return to Netherfield for the entire winter; a report that highly upset Mrs. Bennet and which she never failed to deny as a scandalous falsehood.\n",
      "\n",
      "Metrics for this example:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bertscore</th>\n",
       "      <th>bleu_diversity</th>\n",
       "      <th>edit_distance</th>\n",
       "      <th>syntactic_diversity</th>\n",
       "      <th>harmonic_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BART</th>\n",
       "      <td>0.961170</td>\n",
       "      <td>0.248545</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.330106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T5</th>\n",
       "      <td>0.997983</td>\n",
       "      <td>0.039650</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepSeek-LoRA</th>\n",
       "      <td>0.958008</td>\n",
       "      <td>0.630102</td>\n",
       "      <td>0.358621</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.558361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               bertscore  bleu_diversity  edit_distance  syntactic_diversity  \\\n",
       "BART            0.961170        0.248545       0.189655             0.150000   \n",
       "T5              0.997983        0.039650       0.017241             0.000000   \n",
       "DeepSeek-LoRA   0.958008        0.630102       0.358621             0.157895   \n",
       "\n",
       "               harmonic_score  \n",
       "BART                 0.330106  \n",
       "T5                   0.038877  \n",
       "DeepSeek-LoRA        0.558361  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Domain: TECHNICAL\n",
      "Original: Learn Once, Write Anywhere: We don't make assumptions about the rest of your technology stack, so you can develop new features in React without rewriting existing code.\n",
      "\n",
      "Paraphrases:\n",
      "BART: Paraphrase: Learn once, write anywhere: We don't make assumptions about the rest of your technology stack, so you can develop new features in React without rewriting existing code.\n",
      "T5: Learn Once, Write Anywhere: We don't make assumptions about the rest of your technology stack, so you can develop new features in React without rewriting existing code.\n",
      "DeepSeek-LoRA: We don't make assumptions about the rest of your technology stack, so you can develop new features in React without rewriting existing code.\n",
      "\n",
      "Metrics for this example:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bertscore</th>\n",
       "      <th>bleu_diversity</th>\n",
       "      <th>edit_distance</th>\n",
       "      <th>syntactic_diversity</th>\n",
       "      <th>harmonic_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BART</th>\n",
       "      <td>0.967452</td>\n",
       "      <td>0.060665</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.112539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepSeek-LoRA</th>\n",
       "      <td>0.959586</td>\n",
       "      <td>0.206077</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.277000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               bertscore  bleu_diversity  edit_distance  syntactic_diversity  \\\n",
       "BART            0.967452        0.060665       0.066667             0.058824   \n",
       "T5              1.000000        0.000000       0.000000             0.000000   \n",
       "DeepSeek-LoRA   0.959586        0.206077       0.166667             0.117647   \n",
       "\n",
       "               harmonic_score  \n",
       "BART                 0.112539  \n",
       "T5                   0.000000  \n",
       "DeepSeek-LoRA        0.277000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Domain: ACADEMIC\n",
      "Original: Subsequently, we introduce our infrastructures, encompassing our compute clusters, the training framework, the support for FP8 training, the inference deployment strategy, and our suggestions on future hardware design.\n",
      "\n",
      "Paraphrases:\n",
      "BART: Paraphrase: Subsequently, we introduce our infrastructures, which include our compute clusters, the training framework, the support for FP8 training, the inference deployment strategy and our suggestions on future hardware design.\n",
      "T5: Subsequently, we introduce our infrastructures, encompassing our compute clusters, the training framework, the support for FP8 training, the inference deployment strategy, and our suggestions on future hardware design.\n",
      "DeepSeek-LoRA: Following that, we present our infrastructure, which includes our compute clusters, the training framework, support for FP8 training, the inference deployment strategy, and our recommendations on future hardware design.\n",
      "\n",
      "Metrics for this example:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bertscore</th>\n",
       "      <th>bleu_diversity</th>\n",
       "      <th>edit_distance</th>\n",
       "      <th>syntactic_diversity</th>\n",
       "      <th>harmonic_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BART</th>\n",
       "      <td>0.968842</td>\n",
       "      <td>0.170717</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.256562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepSeek-LoRA</th>\n",
       "      <td>0.963042</td>\n",
       "      <td>0.330498</td>\n",
       "      <td>0.219178</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.415937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               bertscore  bleu_diversity  edit_distance  syntactic_diversity  \\\n",
       "BART            0.968842        0.170717       0.108696                0.125   \n",
       "T5              1.000000        0.000000       0.000000                0.000   \n",
       "DeepSeek-LoRA   0.963042        0.330498       0.219178                0.200   \n",
       "\n",
       "               harmonic_score  \n",
       "BART                 0.256562  \n",
       "T5                   0.000000  \n",
       "DeepSeek-LoRA        0.415937  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Domain: ARTICLE\n",
      "Original: Once they’re reliable enough, they’ll become the only responsible choice for almost all important tasks, ranging from legal rulings and financial planning to healthcare decisions.\n",
      "\n",
      "Paraphrases:\n",
      "BART: Paraphrase: Once they're reliable enough, they'll become the only responsible choice for almost all important tasks, ranging from legal rulings and financial planning to healthcare decisions.\n",
      "T5: Once they’re reliable enough, they’ll become the only responsible choice for almost all important tasks, ranging from legal rulings and financial planning to healthcare decisions.\n",
      "DeepSeek-LoRA: When they prove to be trustworthy enough, they’ll serve as the only responsible option for nearly all significant tasks, from legal decisions to financial planning and healthcare choices.\n",
      "\n",
      "Metrics for this example:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bertscore</th>\n",
       "      <th>bleu_diversity</th>\n",
       "      <th>edit_distance</th>\n",
       "      <th>syntactic_diversity</th>\n",
       "      <th>harmonic_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BART</th>\n",
       "      <td>0.967384</td>\n",
       "      <td>0.195177</td>\n",
       "      <td>0.073298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepSeek-LoRA</th>\n",
       "      <td>0.967668</td>\n",
       "      <td>0.705679</td>\n",
       "      <td>0.406417</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.601308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               bertscore  bleu_diversity  edit_distance  syntactic_diversity  \\\n",
       "BART            0.967384        0.195177       0.073298             0.000000   \n",
       "T5              1.000000        0.000000       0.000000             0.000000   \n",
       "DeepSeek-LoRA   0.967668        0.705679       0.406417             0.166667   \n",
       "\n",
       "               harmonic_score  \n",
       "BART                 0.177292  \n",
       "T5                   0.000000  \n",
       "DeepSeek-LoRA        0.601308  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def find_example(domain):\n",
    "    \"\"\"Find an example ID that exists in all three result sets\"\"\"\n",
    "    bart_ids = set(item[\"id\"] for item in bart_results[\"detailed\"] if item[\"category\"] == domain)\n",
    "    t5_ids = set(item[\"id\"] for item in t5_results[\"detailed\"] if item[\"category\"] == domain)\n",
    "    deepseek_ids = set(item[\"id\"] for item in deepseek_results[\"detailed\"] if item[\"category\"] == domain)\n",
    "    \n",
    "    common_ids = bart_ids.intersection(t5_ids).intersection(deepseek_ids)\n",
    "    return list(common_ids)[0] if common_ids else None\n",
    "\n",
    "def show_example(domain):\n",
    "    \"\"\"Show paraphrase examples from all models for a given domain\"\"\"\n",
    "    example_id = find_example(domain)\n",
    "    if not example_id:\n",
    "        print(f\"No common example found for {domain} domain\")\n",
    "        return\n",
    "    \n",
    "    bart_item = next(item for item in bart_results[\"detailed\"] if item[\"id\"] == example_id)\n",
    "    t5_item = next(item for item in t5_results[\"detailed\"] if item[\"id\"] == example_id)\n",
    "    deepseek_item = next(item for item in deepseek_results[\"detailed\"] if item[\"id\"] == example_id)\n",
    "    \n",
    "    print(f\"Domain: {domain.upper()}\")\n",
    "    print(f\"Original: {bart_item['original']}\")\n",
    "    print(\"\\nParaphrases:\")\n",
    "    print(f\"BART: {bart_item['generated_paraphrase']}\")\n",
    "    print(f\"T5: {t5_item['generated_paraphrase']}\")\n",
    "    print(f\"DeepSeek-LoRA: {deepseek_item['generated_paraphrase']}\")\n",
    "    \n",
    "    # Show metrics for this example\n",
    "    example_metrics = pd.DataFrame({\n",
    "        'BART': bart_item['scores'],\n",
    "        'T5': t5_item['scores'],\n",
    "        'DeepSeek-LoRA': deepseek_item['scores']\n",
    "    }).T[metrics]\n",
    "    \n",
    "    print(\"\\nMetrics for this example:\")\n",
    "    display(example_metrics)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Show examples from each domain\n",
    "for domain in [\"literary\", \"technical\", \"academic\", \"article\"]:\n",
    "    show_example(domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our evaluation, DeepSeek-LoRA demonstrates superior performance in paraphrase generation across multiple metrics and domains compared to the baseline models (BART and T5):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Semantic Preservation: All three models maintain high semantic similarity (BERTScore > 0.95), with T5 achieving nearly perfect scores but at the cost of minimal rephrasing\n",
    "2. Lexical and Structural Diversity: DeepSeek-LoRA significantly outperforms both baseline models in diversity metrics:\n",
    "\n",
    "- 2.2× higher BLEU diversity than BART (0.51 vs 0.23)\n",
    "- 12× higher BLEU diversity than T5 (0.51 vs 0.04)\n",
    "- 2× higher edit distance than BART (0.34 vs 0.17)\n",
    "- 1.5× higher syntactic diversity than BART (0.15 vs 0.09)\n",
    "\n",
    "3. Overall Balance: DeepSeek-LoRA achieves the best harmonic score (0.47), maintaining a strong balance between meaning preservation and creative rephrasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our paraphrasing project offers several promising directions for future research:\n",
    "\n",
    "### Dataset Improvements\n",
    "Create a larger, more diverse training dataset that includes more specialized technical and academic content\n",
    "\n",
    "### Architecture Exploration\n",
    "- Test State Space Models like Mamba SSM for paraphrase generation\n",
    "- Experiment with different Parameter-Efficient Fine-Tuning techniques\n",
    "- Compare performance across model sizes and architectures\n",
    "\n",
    "### Evaluation Enhancements\n",
    "- Use big name LLMs (Claude, GPT-4o) to score paraphrases on a 1-5 scale (on multiple metric specified like diversity, truthfullness to meaning ...)\n",
    "- Add human evaluation to validate automatic metrics (If can hire people)\n",
    "\n",
    "### New Training alignement\n",
    "- Implement RLHF to better align with human judgments (or DPO on paraphrase pairs)\n",
    "- Explore contrastive learning approaches (teacher could be gpt4o or something / student the deepseek model we have)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMFRw8Q/IyO3cXxjSVtdgsB",
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "swisscom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
